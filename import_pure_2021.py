# Download HTML
import sys
import requests
from lxml import html
# Parse RSS-XML
import xml.etree.ElementTree as ET
# Strip HTML
from bs4 import BeautifulSoup
# Store FILE
import codecs

sys.stdout.reconfigure(encoding='utf-8')

#root = ET.parse('https://research.tudelft.nl/en/organisations/control-operations/publications/?format=rss&page=5').getroot()

def download_list(page, filename):

    if page == 0:
        bibf = codecs.open(filename,'w', 'utf-8')
        bibf.write(u'\ufeff')
        bibf.write('# AUTOGENERATED\n# Import from: https://research.tudelft.nl/en/organisations/control-simulation/publications/\n\n\n')
        bibf.close()

    papernr = 1
    pageno = page
    done = False
    while not done:
        print('- Page',pageno)
        
        #p = requests.get('https://research.tudelft.nl/en/organisations/control-operations/publications/?format=rss&page=%d' % pageno)
        p = requests.get('https://research.tudelft.nl/en/organisations/control-simulation/publications/?format=rss&page=%d' % pageno)

        done = True

        root = ET.fromstring(p.text)
        for pub in root.findall('channel/item'):
            title = pub.findall('title')[0].text
            link = pub.findall('guid')[0].text

            #description = pub.findall('description')[0].text
            #dom = html.fromstring(description)
            #journal  = dom.body.find_class('journal')

            htmltxt = '<html><body>txt</body></html>'
            
            # Try 5 times
            for i in range(0,5):
                try:
                    # p = requests.get(link)
                    # htmltxt = p.text

                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',
                        'Accept-Language': 'en-US,en;q=0.9',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
                    }
                    p = requests.get(link, headers=headers)
                    htmltxt = p.text

                    if p.status_code == 404:
                        print('Page not found', link)
                        break
                    elif p.status_code != 200:
                        print('Error', p.status_code, link)
                        continue
                    else:
                        print('Downloaded', link)

                    break
                except:
                    print('Error attempt',i, link)
                    pass

            # If the page is not found, continue

            dom = html.fromstring(htmltxt)

            # Check if the page is not empty
            if dom is None:
                print('Empty page for', link)
                continue

            if dom.body is None:
                print('Empty body for', link)
                continue


            bib = dom.body.get_element_by_id('cite-BIBTEX')
            
            if bib is None:
                print('No bibtex found for', link)
                continue

            bib = bib.getchildren()

            if len(bib) == 0:
                print('No bibtex found for', link)
                continue

            bib = bib[0]
                        
            print(str(papernr) + ' ',title)

            # open and add, in case of error one can continue
            bibf = codecs.open(filename,'a', 'utf-8')
            #bibf.write('# '+str(pageno)+', '+str(papernr)+'\n')
            bibf.write('# '+title+'\n# '+link+'\n\n')
            
            # dump bibtex into file
            for b in bib.getchildren():
                # Try 5 times to download:
                txt = '<html><body>txt</body></html>'
                for i in range(0,5):
                    try:
                        soup = BeautifulSoup(html.tostring(b),features="lxml")
                        txt = soup.get_text()
                        break
                    except:
                        print('Error attempt',i, link)
                        pass
                
                if '}' in txt:
                    bibf.write('\turl       = "'+ link +'",\n')

                if ' url  ' in txt:
                    bibf.write(txt.replace(' url  ', ' url2 ')+'\n')    
                elif not ' abstract ' in txt:
                    #print(txt)
                    bibf.write(txt+'\n')
            #print('')

            bibf.write('\n')
            bibf.close()


            papernr += 1

            # continue is at least 1 paper was found.
            done = False


        pageno += 1

        # debug: stop after 1 page
        #if pageno >= 1:
        #    done = True

     


# To continue downloading, type a non-zero page.
# page=0 resets the output
download_list(0, './pure/cs.bib')
